import torch
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
from torch import nn, optim
from torchcrf import CRF
from transformers import RobertaModel

device = torch.device("cuda:1")


class LstmNerModel(nn.Module):
    def __init__(self, embedding_size=256, num_tags=41,
                 vocab_size=3675, hidden_size=128,
                 batch_first=True, dropout=0.1):
        super(LstmNerModel, self).__init__()
        self.batch_first = batch_first
        self.embedding = nn.Embedding(vocab_size, embedding_size)

        self.lstm = nn.LSTM(embedding_size, hidden_size // 2,
                            num_layers=2, batch_first=True,
                            bidirectional=True, dropout=dropout)
        for name, param in self.lstm.named_parameters():
            if name.startswith("weight"):
                nn.init.xavier_normal_(param)
            else:
                nn.init.zeros_(param)

        self.fc = nn.Linear(hidden_size, num_tags)
        self.crf = CRF(num_tags, batch_first=True)

    def forward(self, input_tensor, seq_lens):
        # print("seq_lens:", seq_lens)
        input_tensor = self.embedding(input_tensor)
        # print("input_tensor:", input_tensor.size())

        total_length = input_tensor.size(1) if self.batch_first else input_tensor.size(0)
        # print("total_length:", total_length)
        input_packed = pack_padded_sequence(input_tensor, seq_lens, batch_first=self.batch_first, enforce_sorted=False)
        # print("input_packed:", input_packed.data.size())
        output_lstm, hidden = self.lstm(input_packed)
        # print("True")
        # print(output_lstm.size())
        output_lstm, length = pad_packed_sequence(output_lstm, batch_first=self.batch_first,
                                                  total_length=total_length)
        output_fc = self.fc(output_lstm)
        # print("output_fc:", output_fc.size())
        return output_fc

    def compute_loss(self, input_tensor, tags, seq_lens):
        mask = torch.zeros(input_tensor.shape[:2])
        mask = mask.to(device)
        mask = torch.greater(input_tensor, mask).type(torch.cuda.ByteTensor)
        # print("mask_shape_1:", mask)

        output_fc = self.forward(input_tensor, seq_lens)

        loss = -self.crf(output_fc, tags, mask, reduction='mean')
        return loss

    def decode(self, input_tensor, seq_lens):
        out = self.forward(input_tensor, seq_lens)
        mask = torch.zeros(input_tensor.shape[:2])
        mask = mask.to(device)
        mask = torch.greater(input_tensor, mask).type(torch.cuda.ByteTensor)

        predicted_index = self.crf.decode(out, mask)
        return predicted_index


class BertNerModel(nn.Module):
    def __init__(self,
                 num_tags=25,
                 batch_first=True,
                 ):
        super(BertNerModel, self).__init__()
        self.batch_first = batch_first
        self.model = RobertaModel.from_pretrained("hfl/chinese-roberta-wwm-ext")
        self.fc = nn.Linear(768, num_tags)
        self.crf = CRF(num_tags, batch_first=True)

    def forward(self, input_tensor):
        # print("input_tensor_before:", input_tensor.size())
        input_tensor = self.model(input_tensor)
        # print("input_tensor_after:", input_tensor.last_hidden_state.size())
        input_tensor = self.fc(input_tensor.last_hidden_state)
        # print("input_tensor_final:", input_tensor.size())
        return input_tensor

    def compute_loss(self, input_tensor, tags):
        # print('input_tensor_shape:', input_tensor.shape)
        mask = torch.zeros(input_tensor.shape[:2])
        # print("mask_shape:", mask)  # [2, 50]
        if torch.cuda.is_available():
            mask = mask.to('cuda:2')
            mask = torch.greater(input_tensor, mask).type(torch.cuda.ByteTensor)
            # print("mask_shape_1:", mask)
        else:
            mask = torch.greater(input_tensor, mask).type(torch.ByteTensor)

        output = self.forward(input_tensor)
        # print('output_size:', output.size())
        # print("tags:", tags)
        # output - bert的输出
        # tags - 每一个字符对应的BIESO标签
        # mask - 标记每一个位置上的字符是否为填补字符
        loss = -self.crf(output, tags, mask, reduction='mean')
        # print("loss of batch_size:", loss.item())
        return loss

    def decode(self, input_tensor):
        # print("decode")
        out = self.forward(input_tensor)
        mask = torch.zeros(input_tensor.shape[:2])
        if torch.cuda.is_available():
            mask = mask.to('cuda:2')
            mask = torch.greater(input_tensor, mask).type(torch.cuda.ByteTensor)
        else:
            mask = torch.greater(input_tensor, mask).type(torch.ByteTensor)

        predicted_index = self.crf.decode(out, mask)
        print('predicted_index:', predicted_index)
        # print('predicted_index:', predicted_inde)
        return predicted_index


class BertLSTMNerModel(nn.Module):
    def __init__(self,
                 num_tags=25,
                 batch_first=True,
                 ):
        super(BertLSTMNerModel, self).__init__()
        self.batch_first = batch_first
        self.hidden_size = 128
        self.num_layers = 1
        self.model = RobertaModel.from_pretrained("hfl/chinese-roberta-wwm-ext")
        self.lstm = nn.LSTM(768, self.hidden_size, bidirectional=True)
        self.fc = nn.Linear(self.hidden_size * 2, num_tags)
        self.crf = CRF(num_tags, batch_first=True)

    def forward(self, input_tensor):
        # print('size_size:', input_tensor.size())
        hidden_state = torch.zeros(self.num_layers * 2, input_tensor.size(1), 128)
        cell_state = torch.zeros(self.num_layers * 2, input_tensor.size(1), 128)
        if torch.cuda.is_available():
            hidden_state = hidden_state.to('cuda')
            cell_state = cell_state.to('cuda')
        # print("input_tensor_before:", input_tensor.size())
        input_tensor = self.model(input_tensor)
        # print("input_tensor_after:", input_tensor.last_hidden_state.size())
        input_tensor, (_, _) = self.lstm(input_tensor.last_hidden_state, (hidden_state, cell_state))
        # print('lstm:', input_tensor.size())
        input_tensor = self.fc(input_tensor)
        # print("input_tensor_final:", input_tensor.size())
        return input_tensor

    def compute_loss(self, input_tensor, tags):
        # print('input_tensor_shape:', input_tensor.shape)
        mask = torch.zeros(input_tensor.shape[:2])
        # print("mask_shape:", mask)  # [2, 50]
        if torch.cuda.is_available():
            mask = mask.to('cuda')
            mask = torch.greater(input_tensor, mask).type(torch.cuda.ByteTensor)
            # print("mask_shape_1:", mask)
        else:
            mask = torch.greater(input_tensor, mask).type(torch.ByteTensor)

        output = self.forward(input_tensor)
        # print('output_size:', output.size())
        # print("tags:", tags)
        # output - bert的输出
        # tags - 每一个字符对应的BIESO标签
        # mask - 标记每一个位置上的字符是否为填补字符
        loss = -self.crf(output, tags, mask, reduction='mean')
        # print("loss of batch_size:", loss.item())
        return loss

    def decode(self, input_tensor):
        # print("decode")
        out = self.forward(input_tensor)
        mask = torch.zeros(input_tensor.shape[:2])
        if torch.cuda.is_available():
            mask = mask.to('cuda')
            mask = torch.greater(input_tensor, mask).type(torch.cuda.ByteTensor)
        else:
            mask = torch.greater(input_tensor, mask).type(torch.ByteTensor)

        predicted_index = self.crf.decode(out, mask)
        # print('predicted_index:', predicted_index)
        return predicted_index

