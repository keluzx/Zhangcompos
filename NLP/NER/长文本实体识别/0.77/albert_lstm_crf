from DATA_LSTM_POS_only import *
from model import LstmCRFAttnPosOnlyModel
import collections
from transformers import AutoTokenizer, BertTokenizer
from lstm_crf import decode_prediction
import os
from torch.nn import DataParallel


device_lstm_crf_attn_pos_only = torch.device("cuda:0")
# gpus = [6, 1, 2]

tokenizer = BertTokenizer.from_pretrained('clue/albert_chinese_tiny')
vocab = tokenizer.get_vocab()
# print('vocab_len:', len(vocab))
train_dev_data = TrainDevData(vocab=vocab)
id2tag = train_dev_data.id2tag
id2char = train_dev_data.id2char
train_dataloader = train_dev_data.train_dataloader
eval_dataloader = train_dev_data.eval_dataloader

vocab_size = train_dev_data.vocab_size
num_tags = train_dev_data.num_tags
lr = 2e-4
embedding_size = 312
hidden_size = 128

model = LstmCRFAttnPosOnlyModel(embedding_size=embedding_size, hidden_size=hidden_size,
                                vocab_size=vocab_size, num_tags=num_tags)
# model = DataParallel(net, device_ids=gpus, output_device=gpus[0])
model.to(device_lstm_crf_attn_pos_only)

params = [{"params": [], 'lr': lr}, {'params': [], 'lr': 100 * lr}]
for p in model.named_parameters():
    if "crf" in p[0]:
        params[1]['params'].append(p[1])
    else:
        params[0]['params'].append(p[1])
'''

'''
optimizer = torch.optim.Adam(params)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.6)


def eval(model=model, eval_dataloader=eval_dataloader):
    model.eval()
    result = {}
    for index, (input_tensor, true_tags, seq_lens, tfidf_weight, pos_weight, seg_weight) in enumerate(eval_dataloader):
        # if torch.cuda.is_available():
        input_tensor = input_tensor.to(device_lstm_crf_attn_pos_only)
        predict_tags = model.decode(input_tensor, seq_lens, tfidf_weight, pos_weight, seg_weight)
        true_tags = list(true_tags.numpy())
        input_tensor = list(input_tensor.cpu().numpy())
        for pre, true, input in zip(predict_tags, true_tags, input_tensor):
            pre = [id2tag[t] for t in pre]
            true = [id2tag[t] for t in true]
            chars = [id2char[c] for c in input if c != 0]
            true = true[:len(chars)]
            pre_result = decode_prediction(chars, pre)
            true_result = decode_prediction(chars, true)
            for type, cnt in pre_result.items():
                if type not in result:
                    result[type] = [0, 0, 0]
                result[type][1] += len(cnt)
                if type in true_result:
                    result[type][0] += len(pre_result[type] & true_result[type])
            for type, cnt in true_result.items():
                if type not in result:
                    result[type] = [0, 0, 0]
                result[type][2] += len(cnt)

    for type, (x, y, z) in result.items():
        X, Y, Z = 1e-10, 1e-10, 1e-10
        X += x
        Y += y
        Z += z

        f1, precision, recall = 2 * X / (Y + Z), X / Y, X / Z
        result[type].append(round(precision, 3))
        result[type].append(round(recall, 3))
        result[type].append(round(f1, 3))
    result = [(k, v) for k, v in result.items()]
    macro_f1 = sum([v[1][-1] for v in result]) / len(result)
    print("macrof1 {}".format(macro_f1))
    result.sort()
    model.train()
    return result


def train(model=model, train_loader=train_dataloader, optimizer=optimizer, scheduler=scheduler, epoch=1000):
    model.train()
    model.to(device_lstm_crf_attn_pos_only)
    for i in range(epoch):
        epoch_loss = 0
        epoch_count = 0
        before = -1
        for index, (input_tensor, tags, seq_lens, tfidf_weight, pos_weight, seg_weight) in enumerate(train_loader):
            input_tensor = input_tensor.to(device_lstm_crf_attn_pos_only)
            tags = tags.cuda()
            loss = model.compute_loss(input_tensor, tags, seq_lens, tfidf_weight, pos_weight, seg_weight)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            epoch_count += input_tensor.shape[0]

            if index % 100 == 0:
                # print(round(epoch_loss / epoch_count, 3))
                print('Loss:', epoch_loss, epoch_count)
                cur = epoch_loss / epoch_count
                # print("cur:", cur)
                # if cur < 0.2 and (before - cur) / before > 0.01:
                result = eval(model, eval_dataloader)
                print(i, index, result)
                if cur < before:
                    before = cur

        scheduler.step()


if __name__ == '__main__':
    train()
