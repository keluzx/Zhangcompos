import time

import torch
import codecs
import numpy as np
import torch.nn as nn
from torch.utils.data import Dataset
from transformers import BertTokenizer
from torch.utils.data import DataLoader
from tqdm import tqdm
import jieba
import jieba.posseg as psg

# import os
# os.environ['CUDA_VISIBLE_DEVICES'] = '0'

device = torch.device("cuda:3" if torch.cuda.is_available() else "cpu")


class CNewsDataset(Dataset):
    def __init__(self, filename):
        # print("filename:", filename)
        # 数据集初始化
        # self.labels = ['体育', '娱乐', '家居', '房产', '教育', '时尚', '时政', '游戏', '科技', '财经']
        self.tokenizer = BertTokenizer.from_pretrained('clue/albert_chinese_tiny')
        self.input_ids = []
        self.input_poses = []
        self.input_vocabulary = []
        self.token_type_ids = []
        self.attention_mask = []
        self.labels2id = []
        self.pos2id = {}
        self.vocab2id = {}
        self.label_dict = {}
        self.vocab_dict = {}
        self.load_data(filename)

    def load_data(self, filename):
        # 加载数据
        print('loading data from:', filename)

        # 遍历filename 并返回filename为内容的列表
        # 原始文本序列化、文本标签序列化
        pos_dict = {}

        sum_times = 0
        for line in open(filename):
            sum_times += 1
        times = 0
        start_time = time.time()
        for line in open(filename):
            times += 1
            if times % 5000 == 0:
                print("times:", times, '/', sum_times)
            label, text = line.strip().split('\t')
            # print("line:", line)
            # print("label:", label)
            if label not in self.label_dict:
                self.label_dict[label] = len(self.label_dict)
            # print("label_dict:", self.label_dict)

            self.labels2id.append(self.label_dict[label])

            token = self.tokenizer(text, add_special_tokens=True, padding='max_length', truncation=True, max_length=256)
            # 词性特征
            text_pos = psg.lcut(text)

            text_vocab = jieba.lcut(text)
            # print("text_vocab:", text_vocab)

            # 词性字典
            original_pos = []
            for index, (word, word_pos) in enumerate(text_pos):
                original_pos.append(word_pos)
                if word_pos not in pos_dict:
                    pos_dict[word_pos] = len(pos_dict)

            # 每一个字符对应的词性在词性字典中的编码
            for i, j in text_pos:
                if j not in self.pos2id:
                    self.pos2id[j] = len(self.pos2id)

            # 词汇字典
            original_vocab = []
            for index, word_vocab in enumerate(text_vocab):
                original_vocab.append(word_vocab)
                if word_vocab not in self.vocab_dict:
                    self.vocab_dict[word_vocab] = len(self.vocab_dict)
            # print("vocab_dict:", len(self.vocab_dict))

            # 每一个字符对应的词性在词性字典中的编码
            for j in text_vocab:
                if j not in self.vocab2id:
                    self.vocab2id[j] = len(self.vocab2id)

            # 加入词性特征输入模型
            input_pos = []
            for i, j in text_pos:
                for a in i:
                    input_pos.append(self.pos2id[j])

            # 加入词汇特征输入模型
            input_vocab = []
            for i in text_vocab:
                for a in i:
                    input_vocab.append(self.vocab2id[i])

            if len(input_pos) > 256:
                input_pos = input_pos[0: 256]
            else:
                while len(input_pos) < 256:
                    input_pos.append(0)

            if len(input_vocab) > 256:
                input_vocab = input_vocab[0: 256]
            else:
                while len(input_vocab) < 256:
                    input_vocab.append(0)

            self.input_ids.append(np.array(token['input_ids']))
            self.token_type_ids.append(np.array(token['token_type_ids']))
            self.attention_mask.append(np.array(token['attention_mask']))
            self.input_vocabulary.append(np.array(input_vocab))
            self.input_poses.append(np.array(input_pos))
            # self.labels2id.append(label2id)
            # print('there', len(self.input_ids))
        print('labels2id_there', len(self.vocab_dict))
        # for i in range(len(self.input_ids)):
        #     print('LENGTH_:', len(self.input_ids[i]))

        data_load_time = time.time() - start_time

        print('data load has finished!')
        print('time used for data load is:', data_load_time / 3600, 'h')

    def __getitem__(self, index):

        return torch.tensor(self.input_ids[index]), torch.tensor(self.labels2id[index]), torch.tensor(self.token_type_ids[index]), \
               torch.tensor(self.attention_mask[index]), torch.tensor(self.input_poses[index]), torch.tensor(self.input_vocabulary[index])

    def __len__(self):
        return len(self.input_ids)


# train_dataset = CNewsDataset('data/demo.txt')
# train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)
