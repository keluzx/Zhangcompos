import torch
from torch.utils.data import DataLoader, Dataset, sampler
from torch.nn.utils.rnn import pad_sequence
import json
import re
import codecs
from random import *
import jieba
import jieba.posseg as psg
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer


device_lstm_crf_attn_pos_tfidf = torch.device("cuda:1")

class CustomNerDataset(Dataset):
    # 首先构造训练集的字典与标签  subtask1_training_part1.txt
    def __init__(self, file_path='data/subtask1_train/subtask1_training_part1.txt', vocab=None, tags=None):
        self.data = []
        self.PAD = "[PAD]"
        self.UNK = "[UNK]"
        # print("file_path:", file_path)
        # print("vocab:", vocab)
        if vocab:
            self.char2id = vocab
            self.char2id_new = vocab
        else:
            self.char2id = {self.PAD: 0, self.UNK: 1}
            self.char2id_new = {self.PAD: 0, self.UNK: 1}
        if tags:
            self.label2id = tags
            self.label2id_new = tags
        else:
            self.label2id = {'O': 0}
            self.label2id_new = {'O': 0}

        word_labels = {}

        sen = ''
        tf_idf_list = []
        total_len_list = []
        sub_vector = None
        char_dict_list = []
        total_start_list, total_end_list = [], []
        len_line = 0
        for line in codecs.open(file_path, 'r', 'utf_8_sig'):
            len_line += 1
            char_jieba_list = []
            char_jieba_list_1 = []
            one_example_jieba = []
            line = json.loads(line)

            # tf_idf
            original_text = line['originalText']
            original_text_jieba = jieba.lcut(original_text)
            char_len_list = []
            for i in original_text_jieba:
                char_len_list.append(len(i))
                sen = sen + i + ' '
            total_len_list.append(char_len_list)
            # sen = "'" + sen + "'"
            # print("sen:", sen)
            tf_idf_list.append(sen)
            # print("tf_idf_list:", tf_idf_list)
            chars_new = []
            for c in line['originalText']:
                chars_new.append(c)
            i_list, word_list = [], []
            for i, word in enumerate(chars_new):
                i_list.append(i)
                word_list.append(word)
            char_dict = dict(zip(i_list, word_list))
            char_dict_list.append(char_dict)
            char_dict_another = dict(zip(word_list, i_list))
            #
            original_text = line.get('originalText')
            entities_text = line.get('entities')
            labels_list, start_end_labels = [], []
            for k in entities_text:
                char_label = k.get('label_type')
                start_label = k.get('start_pos')
                end_label = k.get('end_pos')
                s_e_label = [start_label, end_label]
                start_end_labels.append(s_e_label)
                labels_list.append(char_label)
            # print("labels_list:", len(labels_list))
            cv = CountVectorizer(stop_words=None)
            # 该类会统计每个词语的tf-idf权值
            tf_idf_transformer = TfidfTransformer()
            tf_idf = tf_idf_transformer.fit_transform(cv.fit_transform(tf_idf_list))
            x_train_weight = tf_idf.toarray()

            # tf_idf 的字典
            vector_tf_idf = []
            character_tf_idf = []
            sentence_tf_idf = ''
            sub_vocabulary = cv.vocabulary_
            # tf-idf 字典
            for i in sub_vocabulary.keys():
                character_tf_idf.append(i)
                sentence_tf_idf += i
            # tf-idf字典中每个字符的tf-idf权值
            for i in range(len(x_train_weight[0])):
                vector_tf_idf.append(x_train_weight[0][i])
            character_vector_dict = dict(zip(character_tf_idf, vector_tf_idf))

            # 给所有字符加上tf_idf权重
            for i in range(len(char_dict_list)):
                sub_vector = [0] * len(char_dict_list[i])
                for index, vector in character_vector_dict.items():
                    for key, value in char_dict_list[i].items():
                        for a in index:
                            if a == value:
                                sub_vector[key] = vector
                # vocabulary_tf_idf.append(sub_vector)

            start_list, end_list, s_e_list = [], [], []
            start_values, end_values = [], []
            for i, j in start_end_labels:
                start_list.append(i)
                end_list.append(j)
                s_e_list.append([i, j])
                # print("i_index:", i)
                start_value = char_dict[i]
                end_value = char_dict[j]
                start_values.append(start_value)
                end_values.append(end_value)
            # print("start_list:", s_e_list)
            total_end_list.append(end_list)
            total_start_list.append(start_list)

            start_end_dict_jieba = {}
            for i in range(len(start_end_labels)):
                start_end_dict_jieba[i] = start_end_labels[i]

            original_text_chinese = ''.join(original_text)
            original_text_pos = psg.lcut(original_text_chinese)
            # print("original_text_pos:", original_text_pos)
            char_word, char_pos = [], []
            for i, j in original_text_pos:
                char_word.append(i)

            no_dropped_list = []
            for i in range(len(char_dict.keys())):
                for j in s_e_list:
                    if j[0] <= i <= j[1]:
                        no_dropped_list.append(i)

            original_pos = []
            for index, (word, word_pos) in enumerate(original_text_pos):
                original_pos.append(word_pos)
                if word_pos not in word_labels:
                    word_labels[word_pos] = len(word_labels)

            for word, word_pos in original_text_pos:
                for i in word:
                    for j in no_dropped_list:
                        if char_dict_another[i] == j:
                            word_labels[word_pos] += 1

            word_jieba = []
            word_jieba_demo = []
            char_dropped_len = 0
            sum_dropped_len = 0
            for index, (word, word_pos) in enumerate(original_text_pos):
                if 'n' in word_pos:
                    for i in word:
                        word_jieba.append(i)
                        if i not in self.char2id:
                            self.char2id[i] = len(self.char2id) + char_dropped_len
                elif 'w' in word_pos:  # 要丢弃的字符  'u' and 'b' and   + 'u' or 'wp' or 'ws' or 'a' in
                    for i in word:
                        if random() < 0.1 and char_dict_another[i] not in no_dropped_list:
                            char_dropped_len = len(word)
                            sum_dropped_len += char_dropped_len
                            if i not in self.char2id:
                                self.char2id[i] = len(self.char2id) + char_dropped_len
                    else:
                        for i in word:
                            word_jieba.append(i)
                            if i not in self.char2id:
                                self.char2id[i] = len(self.char2id) + char_dropped_len
                else:
                    for i in word:
                        word_jieba.append(i)
                        if i not in self.char2id:
                            self.char2id[i] = len(self.char2id) + char_dropped_len

            for i in original_text:
                word_jieba_demo.append(i)  # word_jieba - 词汇

            for sub_word_jieba in word_jieba_demo:
                for i in sub_word_jieba:
                    char_jieba_list_1.append(i)

            for sub_word_jieba in word_jieba:
                for i in sub_word_jieba:
                    char_jieba_list.append(i)  # char_jieba_list - 单个字符

            chars_jieba = []
            chars_jieba_1 = []
            for c_jieba in char_jieba_list:
                chars_jieba.append(c_jieba)

            for c_jieba in char_jieba_list_1:
                chars_jieba_1.append(c_jieba)

            one_example_jieba.append(chars_jieba)
            labels_jieba = ['O'] * len(chars_jieba)
            labels_jieba_long = ['O'] * len(chars_jieba_1)

            temp = 0
            for k in labels_list:
                if not tags:
                    if 'B-' + k not in self.label2id:
                        self.label2id['B-' + k] = len(self.label2id)
                        self.label2id['I-' + k] = len(self.label2id)
                        self.label2id['E-' + k] = len(self.label2id)
                        self.label2id['S-' + k] = len(self.label2id)

                start, end = start_end_dict_jieba[temp]
                temp += 1
                assert start <= end
                if start < len(labels_jieba) and end < len(labels_jieba):
                    if start == end:
                        labels_jieba[start] = 'S-' + k
                        # labels_jieba.extend(labels_jieba_long[start])
                    elif start + 1 == end:
                        labels_jieba[start:end + 1] = ['B-' + k, 'E-' + k]
                    else:
                        labels_jieba[start] = 'B-' + k
                        for i in range(start + 1, end):
                            labels_jieba[i] = 'I-' + k
                        labels_jieba[end] = 'E-' + k
                else:
                    if start == end:
                        labels_jieba[start - sum_dropped_len] = 'S-' + k
                    elif start + 1 == end:
                        labels_jieba[start - sum_dropped_len:end - sum_dropped_len + 1] = ['B-' + k, 'E-' + k]
                    else:
                        labels_jieba[start - sum_dropped_len] = 'B-' + k
                        for i in range(start + 1, end):
                            labels_jieba[i - sum_dropped_len] = 'I-' + k

            one_example_jieba.append(labels_jieba)
            one_example_jieba.append(sub_vector)

            self.data.append(one_example_jieba)
            # print('data_data:', self.data)
            print("line:", len_line)
        print('data_data:', self.data)
        print('There')

    # 返回训练集的长度
    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        print('data_data:', self.data)
        chars, labels, tf_idf_weight = self.data[idx]
        chars = [self.char2id[c] if c in self.char2id else self.char2id[self.UNK] for c in chars]
        labels = [self.label2id[lbl] for lbl in labels]

        return {
            'chars': torch.LongTensor(chars),
            'labels': torch.LongTensor(labels),
            'len_chars': len(chars),
            'tf_idf_weight': torch.FloatTensor(tf_idf_weight)
        }


class TrainDevData:
    def __init__(self, train_path="data/subtask1_train/subtask1_training_part1.txt",
                 dev_path="data/subtask1_train/subtask1_training_part1.txt", vocab=None):
        # subtask1_test/subtask1_test_set_with_answer.json

        self.train_data = CustomNerDataset(train_path, vocab=vocab)
        print('len_data:', len(self.train_data))
        self.eval_data = CustomNerDataset(dev_path,
                                          vocab=self.train_data.char2id,
                                          tags=self.train_data.label2id)
        self.train_size = int(len(self.train_data) * 0.7)
        self.validate_size = int(len(self.train_data) * 0.2)
        self.test_size = len(self.train_data) - self.validate_size - self.train_size
        print('train_size:', self.train_size)
        print('validate_size:', self.validate_size)
        print('test_size:', self.test_size)
        self.train_dataset, self.validate_dataset, self.test_dataset = \
            torch.utils.data.random_split(self.train_data, [self.train_size, self.validate_size, self.test_size])

        self.id2char = {v: k for k, v in self.train_data.char2id.items()}
        self.id2tag = {v: k for k, v in self.train_data.label2id.items()}
        self.vocab_size = len(self.train_data.char2id) + 100
        self.num_tags = len(self.train_data.label2id)
        print('num_tags:', self.num_tags)
        print('id2tag:', self.id2tag)
        # 加载训练集  #
        '''
        batch_size:80/10, lr:0.01, char:u,b     F1:0.65
        batch_size:80/20, lr:0.01, char:u,b,w   F1:0.70
        '''
        self.train_dataloader = DataLoader(self.train_dataset, batch_size=80,
                                           shuffle=False, collate_fn=self.len_collate_fn)
        # 加载验证集
        self.eval_dataloader = DataLoader(self.validate_dataset, batch_size=10,
                                          collate_fn=self.len_collate_fn, shuffle=False)

        self.test_dataloader = DataLoader(self.test_dataset, batch_size=20,
                                          collate_fn=self.len_collate_fn, shuffle=True)

    def len_collate_fn(self, batch_data):
        chars, labels, seq_lens = [], [], []
        # print("batch_data:", batch_data)
        for d in batch_data:
            chars.append(d['chars'])
            labels.append(d['labels'])
            seq_lens.append(d['len_chars'])
        # print("chars_batch_size:", chars)
        # print("chars_ss:", chars)
        chars = pad_sequence(chars, batch_first=True, padding_value=0)
        # print("chars:", chars)
        labels = pad_sequence(labels, batch_first=True, padding_value=0)
        # print("data_char:", chars.data.size())
        return chars, labels, torch.LongTensor(seq_lens)


if __name__ == '__main__':
    dataset = CustomNerDataset()
    print(len(dataset.char2id))
    print(len(dataset.label2id))
    # print(dataset.data[-1])
