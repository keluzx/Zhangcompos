import torch
import numpy as np
import torch.nn as nn
from torch.utils.data import Dataset
from transformers import BertTokenizer
from torch.utils.data import DataLoader
from tqdm import tqdm
import jieba
import jieba.posseg as psg


class CNewsDataset(Dataset):
    def __init__(self, filename):
        # print("filename:", filename)
        # 数据集初始化
        self.labels = ['体育', '娱乐', '家居', '房产', '教育', '时尚', '时政', '游戏', '科技', '财经']
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
        self.input_ids = []
        self.input_poses = []
        self.token_type_ids = []
        self.attention_mask = []
        self.label_id = []
        self.pos2id = {}
        self.load_data(filename)

    def load_data(self, filename):
        # 加载数据
        print('loading data from:', filename)

        # 遍历filename 并返回filename为内容的列表
        with open(filename, 'r', encoding='utf-8') as rf:
            lines = rf.readlines()

        # 原始文本序列化、文本标签序列化
        pos_dict = {}
        for line in tqdm(lines, ncols=100):
            label, text = line.strip().split('\t')
            label_id = self.labels.index(label)
            token = self.tokenizer(text, add_special_tokens=True, padding='max_length', truncation=True, max_length=512)
            # 词性特征
            text_pos = psg.lcut(text)
            # print("text_pos:", text_pos)
            original_pos = []
            for index, (word, word_pos) in enumerate(text_pos):
                original_pos.append(word_pos)
                if word_pos not in pos_dict:
                    pos_dict[word_pos] = len(pos_dict)

            for i, j in text_pos:
                if j not in self.pos2id:
                    self.pos2id[j] = len(self.pos2id)

            # print("original_text_pos:", self.pos2id)
            input_pos = []
            for i, j in text_pos:
                for a in i:
                    input_pos.append(self.pos2id[j])

            if len(input_pos) > 512:
                input_pos = input_pos[0: 512]
            else:
                while len(input_pos) < 512:
                    input_pos.append(0)

            self.input_poses.append(np.array(input_pos))
            self.input_ids.append(np.array(token['input_ids']))
            self.token_type_ids.append(np.array(token['token_type_ids']))
            self.attention_mask.append(np.array(token['attention_mask']))
            self.label_id.append(label_id)
            # for i in range(len(self.input_poses)):
            #     print('length_pos:', len(self.input_poses[i]))
            #     print('length_text:', len(self.input_ids[i]))


    def __getitem__(self, index):
        return self.input_ids[index], self.token_type_ids[index], self.attention_mask[index],\
               self.label_id[index], self.input_poses[index]

    def __len__(self):
        return len(self.input_ids)


# train_dataset = CNewsDataset('data/cnews/demo_train.txt')
# train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)
