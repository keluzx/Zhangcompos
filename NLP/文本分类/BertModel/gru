import os
import time

import torch
import torch.nn as nn
from transformers import BertTokenizer, AdamW, BertConfig, AlbertConfig
from torch.utils.data import DataLoader
from model import BertClassifier, LstmClassifier, BertGRUClassifier
from dataset import *
# from tqdm import tqdm
# import os
# os.environ['CUDA_VISIBLE_DEVICES'] = '1'


def main():
    # 参数设置
    # batch_size = 2
    device = torch.device("cuda:3" if torch.cuda.is_available() else "cpu")
    epochs = 30
    learning_rate = 5e-5  # Learning Rate不宜太大

    # 获取到dataset
    train_data = CNewsDataset('data/cnews.train.txt')  # data/cnews/demo_train.txt   cnews.train.txt
    valid_dataset = CNewsDataset('data/cnews.test.txt')  # data/cnews/demo_val.txt  traindata

    # train_size = round(len(train_data) * 0.7)
    # validate_size = round(len(train_data) * 0.3)
    # test_size = len(train_data[0]) - validate_size - train_size
    # print("train_size:", train_size)

    emb_size = len(train_data.vocab_dict)
    # train_dataset, validate_dataset = \
    #     torch.utils.data.random_split(train_data, [train_size, validate_size],
    #                                   generator=torch.Generator().manual_seed(36))

    train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)
    eval_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True)

    # 读取BERT的配置文件
    bert_config = AlbertConfig.from_pretrained('clue/albert_chinese_tiny')
    num_labels = len(train_data.label_dict)
    print("num_labels:", num_labels)
    # 初始化模型
    model = BertGRUClassifier(bert_config, num_labels, emb_size)
    model.to(device)
    # 优化器  .to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    # 损失函数
    criterion = nn.CrossEntropyLoss()

    best_acc = 0

    start_time = time.time()
    for epoch in range(1, epochs + 1):
        losses = 0  # 损失
        accuracy = 0  # 准确率

        model.train()

        for index, (input_ids, label_id, token_type_ids, attention_mask, input_pos, input_vocabulary) in enumerate(
                train_dataloader):
            # 梯度清零 , input_pos, input_vocabulary
            model.zero_grad()
            # print("index:", index)
            # print("input_ids:", input_ids)
            # print("label_id:", label_id)
            # 显示训练进度
            # train_bar.set_description('Epoch %i train' % epoch)

            # 传入数据，调用model.forward()
            output = model(
                input_ids=input_ids.to(device),
                token_type_ids=token_type_ids.to(device),
                attention_mask=attention_mask.to(device),
                input_pos=input_pos.to(device),
                input_vocabulary=input_vocabulary.to(device),
            )
            # print("output:", output.size())
            # print("label_id:", label_id.size())
            # 计算loss

            loss = criterion(output, label_id.to(device))
            # print("loss:", loss)
            losses += loss.item()

            pred_labels = torch.argmax(output, dim=1)  # 预测出的label

            acc = torch.sum(pred_labels == label_id.to(device)).item() / len(pred_labels)  # acc
            accuracy += acc
            # print("acc:", acc)
            loss.backward()
            optimizer.step()
            # train_bar.set_postfix(loss=loss.item(), acc=acc)

        average_loss = losses / len(train_dataloader)
        average_acc = accuracy / len(train_dataloader)
        # print("average_loss:", average_loss)
        print('Epoch:', epoch, '\n', '\tTrain ACC:', average_acc, '\tLoss:', average_loss)

        # 验证
        model.eval()
        losses = 0  # 损失
        accuracy = 0  # 准确率
        # valid_bar = tqdm(valid_dataloader, ncols=100)
        # print('# 验证')
        for index, (input_ids, label_id, token_type_ids, attention_mask, input_pos, input_vocabulary) in enumerate(
                eval_dataloader):
            # 梯度清零 , input_pos, input_vocabulary
            model.zero_grad()
            # print("index:", index)
            # print("input_ids:", input_ids)
            # print("label_id:", label_id)
            # 显示训练进度
            # train_bar.set_description('Epoch %i train' % epoch)

            # 传入数据，调用model.forward().to(device)
            output = model(
                input_ids=input_ids.to(device),
                token_type_ids=token_type_ids.to(device),
                attention_mask=attention_mask.to(device),
                input_pos=input_pos.to(device),
                input_vocabulary=input_vocabulary.to(device),
            )
            # print("output:", output)
            # print("label_id:", label_id)
            # print("output:", output.size())
            # print("label_id:", label_id.size())
            # 计算loss
            loss = criterion(output, label_id.to(device))
            # print("loss:", loss).to(device)
            losses += loss.item()

            pred_labels = torch.argmax(output, dim=1)  # 预测出的label

            acc = torch.sum(pred_labels == label_id.to(device)).item() / len(pred_labels)  # acc   .to(device)
            accuracy += acc
            # print("acc:", acc)

        average_loss = losses / len(eval_dataloader)
        average_acc = accuracy / len(eval_dataloader)

        print('\tValid ACC:', average_acc, '\tLoss:', average_loss)

        if not os.path.exists('models'):
            os.makedirs('models')

        # 判断并保存验证集上表现最好的模型
        if average_acc > best_acc:
            best_acc = average_acc
            torch.save(model.state_dict(), 'models/gru/best_model.pkl')
    run_time = time.time() - start_time
    print("run_time:", run_time / 3600, 'h')


if __name__ == '__main__':
    main()
