import torch
import numpy as np
import torch.nn as nn
from torch.utils.data import Dataset
from transformers import BertTokenizer
from torch.utils.data import DataLoader
from tqdm import tqdm
import jieba
import jieba.posseg as psg


class CNewsDataset(Dataset):
    def __init__(self, filename):
        # print("filename:", filename)
        # 数据集初始化
        self.labels = ['体育', '娱乐', '家居', '房产', '教育', '时尚', '时政', '游戏', '科技', '财经']
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
        self.input_ids = []
        self.input_poses = []
        self.token_type_ids = []
        self.attention_mask = []
        self.label_id = []
        self.pos2id = {}
        self.load_data(filename)

    def load_data(self, filename):
        # 加载数据
        print('loading data from:', filename)

        # 遍历filename 并返回filename为内容的列表
        with open(filename, 'r', encoding='utf-8') as rf:
            lines = rf.readlines()

        # 原始文本序列化、文本标签序列化
        pos_dict = {}
        for line in tqdm(lines, ncols=100):
            label, text = line.strip().split('\t')
            label_id = self.labels.index(label)
            token = self.tokenizer(text, add_special_tokens=True, padding='max_length', truncation=True, max_length=512)
            # 词性特征
            text_pos = psg.lcut(text)
            # print("text_pos:", text_pos)
            original_pos = []
            for index, (word, word_pos) in enumerate(text_pos):
                original_pos.append(word_pos)
                if word_pos not in pos_dict:
                    pos_dict[word_pos] = len(pos_dict)

            for i, j in text_pos:
                if j not in self.pos2id:
                    self.pos2id[j] = len(self.pos2id)

            # print("original_text_pos:", self.pos2id)
            input_pos = []
            for i, j in text_pos:
                for a in i:
                    input_pos.append(self.pos2id[j])

            if len(input_pos) > 512:
                input_pos = input_pos[0: 512]
            else:
                while len(input_pos) < 512:
                    input_pos.append(0)

            self.input_poses.append(np.array(input_pos))
            self.input_ids.append(np.array(token['input_ids']))
            self.token_type_ids.append(np.array(token['token_type_ids']))
            self.attention_mask.append(np.array(token['attention_mask']))
            self.label_id.append(label_id)
            # for i in range(len(self.input_poses)):
            #     print('length_pos:', len(self.input_poses[i]))
            #     print('length_text:', len(self.input_ids[i]))


    def __getitem__(self, index):
        return self.input_ids[index], self.token_type_ids[index], self.attention_mask[index],\
               self.label_id[index], self.input_poses[index]

    def __len__(self):
        return len(self.input_ids)


# train_dataset = CNewsDataset('data/cnews/demo_train.txt')
# train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)






# coding: utf-8
# @File: dataset.py
# @Author: HE D.H.
# @Email: victor-he@qq.com
# @Time: 2021/12/09 11:01:32
# @Description:

import torch
import numpy as np
import torch.nn as nn
from torch.utils.data import Dataset
from transformers import BertTokenizer
from torch.utils.data import DataLoader
from tqdm import tqdm
import jieba
import jieba.posseg as psg


class CNewsDataset(Dataset):
    def __init__(self, filename):
        # print("filename:", filename)
        # 数据集初始化
        self.labels = ['体育', '娱乐', '家居', '房产', '教育', '时尚', '时政', '游戏', '科技', '财经']
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
        self.input_ids = []
        self.input_poses = []
        self.input_vocabulary = []
        self.token_type_ids = []
        self.attention_mask = []
        self.label_id = []
        self.pos2id = {}
        self.vocab2id = {}
        self.load_data(filename)

    def load_data(self, filename):
        # 加载数据
        print('loading data from:', filename)

        # 遍历filename 并返回filename为内容的列表
        with open(filename, 'r', encoding='utf-8') as rf:
            lines = rf.readlines()

        # 原始文本序列化、文本标签序列化
        pos_dict = {}
        vocab_dict = {}
        for line in tqdm(lines, ncols=100):
            label, text = line.strip().split('\t')
            label_id = self.labels.index(label)
            token = self.tokenizer(text, add_special_tokens=True, padding='max_length', truncation=True, max_length=512)
            # 词性特征
            text_pos = psg.lcut(text)
            # print("text_pos:", text_pos)
            text_vocab = jieba.lcut(text)
            # print(text_vocab)
            # 词性字典
            original_pos = []
            for index, (word, word_pos) in enumerate(text_pos):
                original_pos.append(word_pos)
                if word_pos not in pos_dict:
                    pos_dict[word_pos] = len(pos_dict)
            # 每一个字符对应的词性在词性字典中的编码
            for i, j in text_pos:
                if j not in self.pos2id:
                    self.pos2id[j] = len(self.pos2id)
            # 词汇字典
            original_vocab = []
            for index, word_vocab in enumerate(text_vocab):
                original_vocab.append(word_vocab)
                if word_vocab not in vocab_dict:
                    vocab_dict[word_vocab] = len(vocab_dict)
            # 每一个字符对应的词性在词性字典中的编码
            for j in text_vocab:
                if j not in self.vocab2id:
                    self.vocab2id[j] = len(self.vocab2id)
            # print("original_text_pos:", self.vocab2id)
            # 加入词性特征输入模型
            input_pos = []
            for i, j in text_pos:
                for a in i:
                    input_pos.append(self.pos2id[j])
            # 加入词汇特征输入模型
            input_vocab = []
            for i in text_vocab:
                for a in i:
                    input_vocab.append(self.vocab2id[i])
            # print("input_vocab:", input_vocab)
            if len(input_pos) > 512:
                input_pos = input_pos[0: 512]
            else:
                while len(input_pos) < 512:
                    input_pos.append(0)

            if len(input_vocab) > 512:
                input_vocab = input_vocab[0: 512]
            else:
                while len(input_vocab) < 512:
                    input_vocab.append(0)

            self.input_vocabulary.append(np.array(input_vocab))
            self.input_poses.append(np.array(input_pos))
            self.input_ids.append(np.array(token['input_ids']))
            self.token_type_ids.append(np.array(token['token_type_ids']))
            self.attention_mask.append(np.array(token['attention_mask']))
            self.label_id.append(label_id)
            # print(len(self.input_poses))
            # for i in range(len(self.input_poses)):
            #     print('length_pos:', len(self.input_poses[i]))
            #     print('length_text:', len(self.input_ids[i]))

    def __getitem__(self, index):
        return self.input_ids[index], self.token_type_ids[index], self.attention_mask[index],\
               self.label_id[index], self.input_poses[index], self.input_vocabulary[index]

    def __len__(self):
        return len(self.input_ids)


train_dataset = CNewsDataset('data/cnews/demo_train.txt')
train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)

