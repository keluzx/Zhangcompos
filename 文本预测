任务目标：监督任务，输入文本某一段内容，输出该内容之后的内容。
对于一段文本，“忘却成都来十载”，输入“忘却成都来”，输出“十载”。

首先应该清楚，模型的输入层、隐藏层与输出层的维度关系：
输入层：[seq_len, batch_size, inut_size]       # seq_len——序列长度，对应该任务为有几句话作为输入    
隐藏层：[num_layers, batch_size, hidden_size]  # num_layers——堆叠几层RNN、batch_size——一句话有几个字或者是一句话分成几个批次
输出层：[seq_len, batch_size, hidden_size]     # hidden_size 隐层节点个数、input_size——每个字使用多少特征表示，也就是一个向量中的列数

实现方法：

import torch
import torch.nn as nn
import torch.optim as optim
import jieba

epochs = 100
embedding_size = 10
hidden_size = 6

def make_batch():
	input_batch, target_batch = [], []
	input_batch = [word_dict[n] for n in enumerate(sentences_cut[:-1])]
	target_batch = [word_dict[sentences_cut[-1]]]
	input_batch_len = len(input_batch)
	
	return input_batch, target_batch, input_batch_len


class TextPred(nn.Module):
	def __init__(self):
		self.emb = nn.Embedding(input_batch_len, embedding_size)                
		self.rnn = nn.RNN(input_size = input_size, hidden_size = hidden_size)  
		self.fc = nn.Linear(input_batch_len, input_size)
	def forward(self, inputs, hidden):
		inputs = self.emb(inputs)
		inputs = inputs.unsqueeze(0)
		output, hidden = self.rnn(inputs, hidden)
		output = output.transpose(0, 1)
		output = output[-1]
		output = self.fc(output)
		
		return output


if __name__ == '__main__':
	sentences = "忘却成都来十载"
	sentences_cut = jieba.lcut(sentences)
	word_dict = {w:i for i, w in enumerate(sentences_cut)}
	num_dict = {i:w for i, w in enumerate(sentences_cut)}
	input_size = len(word_dict)
	
	input_batch, target_batch, input_batch_len = make_batch()
	hidden = torch.zeros(1, input_batch_len, hidden_size)
	model = TextPred()
	criterion = nn.CrossEntrophyLoss()
	optimizer = optim.Adam(model.parameters(), lr=0.001)
	
	for epoch in range(epochs):
		output = model(input_batch)
		loss = criterion(output, target_batch)
		
		if (epoch + 1) % 100 == 0:
			print('Epoch:', '%3d' % (epoch), 'Loss:', '%8d' & (loss.item()))
		
		loss.backward()
		optimizer.step()
		
	
	predict = model(input_batch, hidden).data.max(1, keepdim=True)[1]
	print([sen for sen in sentence_cut[:-1]], '->', [num_dict[predict.item()]] )
	
使用jiebe分词，对原始输入进行分词操作。分词之后的内容为 “忘却”、“成都”、“来”、“十载”。 
构造字典为：{'忘却': 0, '成都': 1, '来': 2, '十载': 3}
	
首先构造输入张量，输入张量维度为[3,4]，即三行四列的矩阵。
每一行分别表示 “忘却”、“成都”、“来” 这几个词，每一个词使用四个特征来表示，也就是四列。为什么使用四列？因为字典长度为4，最终通过连接一个全连接网
络，模型输出具有四个特征的张量，每一个特征上的数值大小对应为该时刻输出为该位置特征的概率大小。比如，如果第四个数最大，那么模型输出就是字典中第四
个字。输入张量构造完之后，还需要目标张量，使用目标张量与模型输出计算损失，通过反向传播，更新梯度优化方向，以减小损失，优化表现效果。目标张量构造
为[3]。“十载”就是目标张量所表示的字符，该字符在字典中索引为3.



		
		
	
	
