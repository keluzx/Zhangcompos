# https://github.com/EliasCai/sentiment 代码持续更新，欢迎大家关注，希望有所帮助，共同提升
# 个人介绍：工作从事大数据开发，熟悉机器学习和深度学习的使用
# 比赛经验：曾参加场景分类（AiChallenger）、口碑商家客流量预测（天池）、用户贷款风险预测（DataCastle）、
#           摩拜算法天战赛（biendata）等，寻找队友冲击前排，希望不吝收留！
# 版本：v1.1
# 环境：python3; tensorflow-1.0.0; keras-2.0.6
# 邮箱：elias8888#qq.com
# 使用：将data文件夹中的三个csv文件放到py文件同个文件夹下面即可运行
# Finish：
# 使用jieba进行分词，并用LSTM对第一个情感关键词进行预测，10轮epochs后验证样本的准确率为0.70
#
# Todo：
# 1、将情感关键词添加到jieba的字典里
# 2、将第2、3个关键词添加到样本，将预测的概率大于阈值的位置作为情感关键词输出
# 3、完成主题和情感正负面的分析
# 4、完善LSTM的网络
# 5、试试CNN的效果

from random import randint
import jieba
import jieba.analyse
import pandas as pd
import codecs
from keras.preprocessing.text import Tokenizer
import matplotlib

import numpy as np
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Embedding, SimpleRNN, Dropout
from keras import regularizers
import matplotlib.pyplot as plt

matplotlib.use('TkAgg')
matplotlib.get_backend()



epochs = 1
jieba.load_userdict(r"train_word.csv")


def getText():
    # 获取文本输入
    contents = []
    words = []
    # codecs.open 用来读取文件 三个参数依次指 1.文件所在路径 2.文件编码方式 3.操作文件的方式
    with codecs.open('train_content.csv', encoding='utf-8', mode='rU') as f:
        # print("Train_content:")
        # print('F:', f)
        for line in f:
            # print("content_line:", line)
            contents.append(line)
    # print("contents:", contents)
    # print('len of contents  :', len(contents))
    with codecs.open('train_word.csv', encoding='utf-8', mode='rU') as f:
        # print("Train_word:")
        for line in f:
            # print("word_line:", line)
            words.append(line)
    # print('len of words : ', len(words))
    return contents, words


def calWordCover(contents, words, k=10):
    # 计算分词结果对情感关键词的覆盖率
    # params
    # contents : 评论内容
    # words ：情感关键词
    # k : 关键词数量，暂时用不上
    # return
    # contents : 分词后重新组成的评论内容
    # words : 将关键词拆分并组成List
    keysLen = 0
    coverLen = 0
    new_contents = []
    new_words = []

    print('关键词 / 被覆盖的关键词')
    for i in range(len(contents)):
        tags = jieba.lcut(contents[i].strip(), cut_all=False)
        sub_tags_dict = {w: i for i, w in enumerate(tags)}
        sub_tags_num_dict = {i: w for i, w in enumerate(tags)}
        inputs_id = [i for i, w in enumerate(sub_tags_dict)]

        n_pred = round(len(inputs_id) * 0.2)
        for pos in inputs_id[:n_pred]:
            index = randint(0, len(inputs_id) - 1)
            inputs_id[pos] = sub_tags_dict[sub_tags_num_dict[index]]

        dict_list = []
        for num in inputs_id:
            word_dict = sub_tags_num_dict[num]
            dict_list.append(word_dict)
        keys = set([i for i in words[i].strip().split(';') if len(i) > 0])  # 去掉换行符、空的关键词和重复词

        keysLen += len(keys)  # keysLen 为情感关键词的总长度 即情感语料train_word中共有多少个情感关键词
        # print("情感关键词长度-keysLen:", keysLen)
        '''
        语料中一个评论样本对应一个情感关键词样本，两种类型的样本量各有2万个，从评论样本中提取情感词汇组成
        情感关键词样本。keys为每一个样本中的情感关键词文本，tags为每一个样本中的切分后的评论文本，要知道
        keys中的词汇是由tags文本提取出来的，在这个任务中，keys是人工提取，keys为tags中的部分词汇-情感词
        如果要查看是否keys中的词汇全部被tags文本覆盖，结论显然。可是，tags是要做文本切分的，不同的切分工具
        切分结果不同，因此keys中的词汇并非一定全部被切分后的tags文本覆盖。
        构建tags文本和keys文本的集合类型文本，查看两者交集情况，若tags文本正确切分，那么在当前此样本中，覆盖率
        应为1，以此类推，对所有样本进行如此操作。coverLen记录每一时刻两种样本的交集词汇个数，到最后时刻，通过
        coverLen / len(keys) 计算最终覆盖率
        '''
        coverLen += len(keys & set(tags))
        # 每一条评论也就是每一个批次或者叫样本 都使用空格作语句切分 将值返回给contents
        contents[i] = ' '.join(list(tags))
        words[i] = list(keys)
        new_contents.append(contents[i])
        new_words.append(words[i])
        new_contents.append(dict_list)

        new_words.append(words[i])
    print("class of new_contents:", type(new_contents))
    print('覆盖率 : ', coverLen / keysLen)
    # 返回的 contents 和 words 均为列表格式，contents已切分完毕
    return new_contents, new_words


# getText() 拿到预料中的评论文本与情感词汇文本
contents, words = getText()
# calWordCover() 返回contents 和 words ，contents为切分完毕的评论文本
# words为情感词汇文本，两者均为列表格式，同时计算分词结果对情感词汇的覆盖率
contents, words = calWordCover(contents, words, 15)

dfWords = pd.DataFrame(words)
print("dfWords:", dfWords)
# print("contents:", contents)
dfContents = pd.DataFrame(contents)
print("dfContents:", dfContents)
print('平均评论样本长度：', dfContents.count(axis=1).mean())

print('平均情感关键词数量 : ', dfWords.count(axis=1).mean())  # 平均情感关键词的数量
# 剔除没有情感关键词的句子

indexWord = dfWords.index[~pd.isnull(dfWords.iloc[:, 0])]  # 第一个关键词非空的index
# print("indexWord:", indexWord)
word = dfWords.iloc[indexWord, 0].tolist()  # 第一个关键词非空的列表

contents = pd.DataFrame(contents).iloc[indexWord, 0].tolist()  # 第一个关键词非空对应的评论内容
print("contents:", pd.DataFrame(contents))
tokenizer = Tokenizer(num_words=100000)
# 使用Tokenizer进行文本预处理，序列化，向量化
# 使用Tokenizer.word_index 可以查看每一个字符与其对应的索引
# 在此任务的contents语料中，字符对应分词后的序列，并非每一个字就是一个字符
# 使用fit_on_texts方法 构建非空的评论文本和情感词汇文本词典
tokenizer.fit_on_texts(contents + word)
# print("contents:", contents)
# 使用text_to_sequence方法 获取评论文本中由字符的索引组成的列表
sequences = tokenizer.texts_to_sequences(contents)
# print("sequences:", sequences)
# 使用text_to_sequence方法 获取情感词汇文本中由字符的索引组成的列表
sequences_words = tokenizer.texts_to_sequences(word)
# 使用pad_sequences方法将sequence每一个序列或填充或切分至maxlen长度
# pad_sequences 在传入的序列从前往后填充0
data_x = pad_sequences(sequences, maxlen=60)  # 平均长度是20，最长是200，设置为50


def getDataY(data_x):
    # data_x 为经过pad与去空值后的评论文本
    # 获取情感关键词在评论内容中的位置
    data_y = []
    for i in range(data_x.shape[0]):
        # print("data_x[i]:", data_x)
        # 对于try语句，若try模块没有报错，则执行try模块下的语句
        # 若try报错，则执行except模块
        # print(list(data_x[i]).index(sequences_words[i][0]))
        # print("word_index:", sequences_words[i][0])
        try:
            data_y.append(list(data_x[i]).index(sequences_words[i][0]))
            # print("data_y:", data_y[i])
        except:
            data_y.append(-1)  # 如果情感关键词不在分词里面
            # print("data_y:", data_y[i])

    # print('data_y_len:', len(data_y))
    return np.array(data_y)


data_y = getDataY(data_x)
print(len(data_y[data_y >= 0]))

# 这里onehot_y编码的第一个维度表示评论文本中有意义的样本个数
# 有意义是指切分后的评论文本含有情感词汇文本中的情感词
# 若不含有 则剔除该评论文本中的该样本
# onehot_y编码的第二个维度表示情感词汇文本中的情感词汇在评论文本中的索引
# 若情感词汇文本中某一个情感词在评论文本中的第三个位置
# 则第三个位置置1 其余为0
onehot_y = to_categorical(data_y[data_y >= 0], num_classes=60)  # 将位置的信息转化为OneHot

train_x, test_x, train_y, test_y = train_test_split(data_x[data_y >= 0], onehot_y, test_size=0.2)


def trainModel(train_x, test_x, train_y, test_y):
    cell_size = 256  # pytorch 中的hidden_size
    time_steps = 64  # 一个batch中有几句话________________________________________
    input_size = 128
    # 训练模型
    model = Sequential()
    model.add(Embedding(len(tokenizer.word_index) + 1, 128))
    model.add(Dropout(0.2, input_shape=(time_steps, input_size)))
    model.add(SimpleRNN(units=cell_size,
                        input_shape=(time_steps, input_size),
                        )
              )
    model.add(Dense(60, activation='softmax',
                    activity_regularizer=regularizers.l1(0.02)
                    ),
              )
    model.summary()
    model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])

    history = model.fit(train_x, train_y,
                        batch_size=64,
                        epochs=epochs,
                        validation_data=(test_x, test_y),

                        shuffle=True)  # 验证集

    fig = plt.figure()
    training_accuracy = history.history['accuracy']
    test_accuracy = history.history['val_accuracy']
    print(test_accuracy)
    plt.plot(range(1, epochs + 1), training_accuracy, "r--")
    plt.plot(range(1, epochs + 1), test_accuracy, "b-")
    plt.legend(["Training Accuracy", "Test Accuracy"])
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy Score")
    plt.title("Fused Samples")
    plt.show()

    model.save('myfile/rnn.hdf5')
    return model


model = trainModel(train_x, test_x, train_y, test_y)
print("train_x size:", train_x.shape)


def getPredWord(model, word_index, x, y):  # x-test_x, y-test_y
    print('1221', x)
    print('1221', x.shape)
    print('1221', x[[1, 2], [1]])
    # 获取评论内容的预测关键词
    pos_pred = model.predict(x)
    print("pos_pred:", pos_pred)
    print("pos_pred size:", pos_pred.shape)
    # argmax 中的axis参数有0和1的取值，0表示消除外层中括号，也就是第一个维度，1表示消除第二个维度
    pos_pred = np.argmax(pos_pred, axis=1)  # argmax返回numpy数组中最大值的索引值
    print("pos_pred:", pos_pred)
    print("pos_pred_len:", len(pos_pred))
    pos_y = np.argmax(y, axis=1)  # 最大值的位置，也就是还原到data_y的形式
    print("pos_y:", pos_y)
    print("pos_y_len:", len(pos_y))
    # x[[i for i in range(pos_pred.shape[0])], pos_pred] # 通过预测的位置获取关键词

    index2word = pd.DataFrame.from_dict(word_index, 'index').reset_index()  # 将word2index转化为index2word

    print('模型预测的关键词')
    # head(n)显示前n行
    print(index2word.loc[x[[i for i in range(pos_pred.shape[0])], pos_pred]].head(5))
    print('实际样本的关键词')
    print(index2word.loc[x[[i for i in range(pos_y.shape[0])], pos_y]].head(5))


getPredWord(model, tokenizer.word_index, test_x, test_y)
