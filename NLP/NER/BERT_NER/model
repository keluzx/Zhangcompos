import torch
import numpy as np
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
from torch import nn, optim
from torchcrf import CRF
from transformers import RobertaModel
import torch.nn.functional as F

device_lstm_crf = torch.device("cuda:0")
device_bert_crf = torch.device("cuda:1")
device_bert_lstm_crf = torch.device("cuda:2")
device_lstm_bert_crf = torch.device("cuda:3")
device_lstm_crf_attn = torch.device("cuda:4")


class LstmCRFModel(nn.Module):
    def __init__(self, embedding_size=256, num_tags=25,
                 vocab_size=3675, hidden_size=128,
                 batch_first=True, dropout=0.1):
        super(LstmCRFModel, self).__init__()
        self.batch_first = batch_first
        self.embedding = nn.Embedding(vocab_size, embedding_size)

        self.lstm = nn.LSTM(embedding_size, hidden_size // 2,
                            num_layers=2, batch_first=True,
                            bidirectional=True, dropout=dropout)
        for name, param in self.lstm.named_parameters():
            if name.startswith("weight"):
                nn.init.xavier_normal_(param)
            else:
                nn.init.zeros_(param)

        self.fc = nn.Linear(128, num_tags)
        self.crf = CRF(num_tags, batch_first=True)

    def forward(self, input_tensor, seq_lens):
        input_tensor = self.embedding(input_tensor)

        total_length = input_tensor.size(1) if self.batch_first else input_tensor.size(0)

        input_packed = pack_padded_sequence(input_tensor, seq_lens, batch_first=self.batch_first, enforce_sorted=False)

        output_lstm, hidden = self.lstm(input_packed)
        # print("input_packed:", hidden[0].size())
        output_lstm, length = pad_packed_sequence(output_lstm, batch_first=self.batch_first,
                                                  total_length=total_length)

        # attn_output = self.attention_net(output_lstm, hidden[0])

        output_fc = self.fc(output_lstm)
        # print("output_fc:", output_fc.size())
        return output_fc

    # def attention_net(self, output_lstm, final_hidden):
    #     hidden = final_hidden.view(-1, 128, 1)
    #     print("attn_weights:", hidden.size())
    #     print("attn_weights:", output_lstm.size())
    #     attn_weights = torch.bmm(output_lstm, hidden).squeeze(2)  # attn_weights : [batch_size, n_step]
    #
    #     soft_attn_weights = F.softmax(attn_weights, 1)
    #     context = torch.bmm(output_lstm.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)
    #
    #     print("context:", context.size())
    #     return context

    def compute_loss(self, input_tensor, tags, seq_lens):
        mask = torch.zeros(input_tensor.shape[:2])
        mask = mask.to(device_lstm_crf)
        mask = torch.greater(input_tensor, mask).type(torch.cuda.ByteTensor)
        # print("mask_shape_1:", mask)

        output_fc = self.forward(input_tensor, seq_lens)

        loss = -self.crf(output_fc, tags, mask, reduction='mean')
        return loss

    def decode(self, input_tensor, seq_lens):
        out = self.forward(input_tensor, seq_lens)
        mask = torch.zeros(input_tensor.shape[:2])
        mask = mask.to(device_lstm_crf)
        mask = torch.greater(input_tensor, mask).type(torch.cuda.ByteTensor)

        predicted_index = self.crf.decode(out, mask)
        return predicted_index


class BertCRFModel(nn.Module):
    def __init__(self,
                 num_tags=25,
                 batch_first=True,
                 ):
        super(BertCRFModel, self).__init__()
        self.batch_first = batch_first
        self.model = RobertaModel.from_pretrained("hfl/chinese-roberta-wwm-ext")
        self.fc = nn.Linear(768, num_tags)
        self.crf = CRF(num_tags, batch_first=True)

    def forward(self, input_tensor):
        # print("input_tensor_before:", input_tensor.size())
        input_tensor = self.model(input_tensor)
        # print("input_tensor_after:", input_tensor.last_hidden_state.size())
        input_tensor = self.fc(input_tensor.last_hidden_state)
        # print("input_tensor_final:", input_tensor.size())
        return input_tensor

    def compute_loss(self, input_tensor, tags):
        # print('input_tensor_shape:', input_tensor.shape)
        mask = torch.zeros(input_tensor.shape[:2])
        # print("mask_shape:", mask)  # [2, 50]
        if torch.cuda.is_available():
            mask = mask.to(device_bert_crf)
            mask = torch.greater(input_tensor, mask).type(torch.cuda.ByteTensor)
            # print("mask_shape_1:", mask)
        else:
            mask = torch.greater(input_tensor, mask).type(torch.ByteTensor)

        output = self.forward(input_tensor)
        # print('output_size:', output.size())
        # print("tags:", tags)
        # output - bert的输出
        # tags - 每一个字符对应的BIESO标签
        # mask - 标记每一个位置上的字符是否为填补字符
        loss = -self.crf(output, tags, mask, reduction='mean')
        # print("loss of batch_size:", loss.item())
        return loss

    def decode(self, input_tensor):
        # print("decode")
        out = self.forward(input_tensor)
        # print('out:', out)
        mask = torch.zeros(input_tensor.shape[:2])
        if torch.cuda.is_available():
            mask = mask.to(device_bert_crf)
            mask = torch.greater(input_tensor, mask).type(torch.cuda.ByteTensor)
        else:
            mask = torch.greater(input_tensor, mask).type(torch.ByteTensor)

        predicted_index = self.crf.decode(out, mask)
        # print('predicted_index:', predicted_index)

        return predicted_index


class BertLSTMNerModel(nn.Module):
    def __init__(self,
                 num_tags=25,
                 batch_first=True,
                 ):
        super(BertLSTMNerModel, self).__init__()
        self.batch_first = batch_first
        self.hidden_size = 128
        self.num_layers = 1
        self.model = RobertaModel.from_pretrained("hfl/chinese-roberta-wwm-ext")
        self.lstm = nn.LSTM(768, self.hidden_size, bidirectional=True)
        self.fc = nn.Linear(self.hidden_size * 2, num_tags)
        self.crf = CRF(num_tags, batch_first=True)

    def forward(self, input_tensor):
        # print('size_size:', input_tensor.size())
        hidden_state = torch.zeros(self.num_layers * 2, input_tensor.size(1), 128)
        cell_state = torch.zeros(self.num_layers * 2, input_tensor.size(1), 128)
        if torch.cuda.is_available():
            hidden_state = hidden_state.to(device_bert_lstm_crf)
            cell_state = cell_state.to(device_bert_lstm_crf)
        # print("input_tensor_before:", input_tensor.size())
        input_tensor = self.model(input_tensor)
        # print("input_tensor_after:", input_tensor.last_hidden_state.size())
        input_tensor, (_, _) = self.lstm(input_tensor.last_hidden_state, (hidden_state, cell_state))
        # print('lstm:', input_tensor.size())
        input_tensor = self.fc(input_tensor)
        # print("input_tensor_final:", input_tensor.size())
        return input_tensor

    def compute_loss(self, input_tensor, tags):
        # print('input_tensor_shape:', input_tensor.shape)
        mask = torch.zeros(input_tensor.shape[:2])
        # print("mask_shape:", mask)  # [2, 50]
        if torch.cuda.is_available():
            mask = mask.to(device_bert_lstm_crf)
            mask = torch.greater(input_tensor, mask).type(torch.cuda.ByteTensor)
            # print("mask_shape_1:", mask)
        else:
            mask = torch.greater(input_tensor, mask).type(torch.ByteTensor)

        output = self.forward(input_tensor)
        # print('output_size:', output.size())
        # print("tags:", tags)
        # output - bert的输出
        # tags - 每一个字符对应的BIESO标签
        # mask - 标记每一个位置上的字符是否为填补字符
        loss = -self.crf(output, tags, mask, reduction='mean')
        # print("loss of batch_size:", loss.item())
        return loss

    def decode(self, input_tensor):
        # print("decode")
        out = self.forward(input_tensor)
        mask = torch.zeros(input_tensor.shape[:2])
        if torch.cuda.is_available():
            mask = mask.to(device_bert_lstm_crf)
            mask = torch.greater(input_tensor, mask).type(torch.cuda.ByteTensor)
        else:
            mask = torch.greater(input_tensor, mask).type(torch.ByteTensor)

        predicted_index = self.crf.decode(out, mask)
        # print('predicted_index:', predicted_index)
        return predicted_index


class LSTMBERTNerModel(nn.Module):
    def __init__(self, embedding_size=256, num_tags=25,
                 vocab_size=3675, hidden_size=128,
                 batch_first=True, dropout=0.1):
        super(LSTMBERTNerModel, self).__init__()
        self.batch_first = batch_first
        self.embedding = nn.Embedding(vocab_size, embedding_size)

        self.lstm = nn.LSTM(embedding_size, hidden_size // 2,
                            num_layers=2, batch_first=True,
                            bidirectional=True, dropout=dropout)
        self.model = RobertaModel.from_pretrained("hfl/chinese-roberta-wwm-ext")

        for name, param in self.lstm.named_parameters():
            if name.startswith("weight"):
                nn.init.xavier_normal_(param)
            else:
                nn.init.zeros_(param)

        self.fc = nn.Linear(hidden_size, num_tags)
        self.crf = CRF(num_tags, batch_first=True)

    def forward(self, input_tensor, seq_lens):
        # print("seq_lens:", seq_lens)
        input_tensor = self.embedding(input_tensor)
        print("input_tensor:", input_tensor.size())

        total_length = input_tensor.size(1) if self.batch_first else input_tensor.size(0)
        # print("total_length:", total_length)
        input_packed = pack_padded_sequence(input_tensor, seq_lens, batch_first=self.batch_first, enforce_sorted=False)
        print("input_packed:", input_packed.data.size())
        output_lstm, hidden = self.lstm(input_packed)
        print("hidden_value:", hidden)
        print("hidden:", hidden[1].size())
        print("hidden:", hidden[0].size())
        # output_lstm = output_lstm.data.to(device_lstm_bert_crf)
        print('output_size:', output_lstm.data.size())
        output_lstm = output_lstm.cpu().data
        print("output_lstm:", output_lstm)
        output_lstm = int(output_lstm.data)
        output_lstm = torch.LongTensor(output_lstm)
        # output_lstm, length = pad_packed_sequence(output_lstm, batch_first=self.batch_first,
        #                                           total_length=total_length)
        output_bert = self.model(output_lstm)

        print("hidden_lstm:", output_bert.last_hidden_state.size())
        output_fc = self.fc(output_lstm)
        print("output_fc:", output_fc)
        return output_fc

    def compute_loss(self, input_tensor, tags, seq_lens):
        mask = torch.zeros(input_tensor.shape[:2])
        mask = mask.to(device_lstm_bert_crf)
        mask = torch.greater(input_tensor, mask).type(torch.cuda.ByteTensor)
        # print("mask_shape_1:", mask)

        output_fc = self.forward(input_tensor, seq_lens)

        loss = -self.crf(output_fc, tags, mask, reduction='mean')
        return loss

    def decode(self, input_tensor, seq_lens):
        out = self.forward(input_tensor, seq_lens)
        mask = torch.zeros(input_tensor.shape[:2])
        mask = mask.to(device_lstm_bert_crf)
        mask = torch.greater(input_tensor, mask).type(torch.cuda.ByteTensor)

        predicted_index = self.crf.decode(out, mask)
        return predicted_index


class LstmCRFAttnModel(nn.Module):
    def __init__(self, embedding_size=256, num_tags=25,
                 vocab_size=3675, hidden_size=128,
                 batch_first=True, dropout=0.1):
        super(LstmCRFAttnModel, self).__init__()
        self.batch_first = batch_first
        self.embedding = nn.Embedding(vocab_size, embedding_size)
        self.embedding_attn = nn.Embedding(vocab_size, hidden_size)
        self.lstm = nn.LSTM(embedding_size, hidden_size // 2,
                            num_layers=2, batch_first=True,
                            bidirectional=True, dropout=dropout)
        for name, param in self.lstm.named_parameters():
            if name.startswith("weight"):
                nn.init.xavier_normal_(param)
            else:
                nn.init.zeros_(param)

        self.fc = nn.Linear(128, num_tags)
        self.crf = CRF(num_tags, batch_first=True)

    def forward(self, input_tensor, seq_lens):
        # print("input_tensor:", input_tensor)
        input_tensor = self.embedding(input_tensor)
        total_length = input_tensor.size(1) if self.batch_first else input_tensor.size(0)
        input_packed = pack_padded_sequence(input_tensor, seq_lens, batch_first=self.batch_first, enforce_sorted=False)
        output_lstm, hidden = self.lstm(input_packed)
        output_lstm, length = pad_packed_sequence(output_lstm, batch_first=self.batch_first,
                                                 total_length=total_length)

        # print("output_lstm:", output_lstm.size())
        # print("final_hidden:", hidden[0].size())
        attn_output = self.attention_net(output_lstm, hidden[0])
        # print("attn_output:", attn_output.size())
        output_fc = self.fc(attn_output)
        # print("output_fc:", output_fc.size())

        return output_fc

    def attention_net(self, output_lstm, final_hidden):
        # print("final_hidden:", final_hidden.size())  # [4, 100, 128]
        hidden = final_hidden.view(-1, 128, 2)  # hidden 为每一个batch的最后一个字符
        # print('hidden:', hidden.size())
        attn_weights = torch.bmm(output_lstm, hidden).squeeze(2)  # attn_weights : [batch_size, n_step]
        # print("attn_weights:", attn_weights.size())

        soft_attn_weights = F.softmax(attn_weights, 1)
        # print(soft_attn_weights.size())
        soft_attn_weights = soft_attn_weights.repeat(1, 1, 64)
        # print("soft_attn_weights:", soft_attn_weights.size())
        context = output_lstm + soft_attn_weights
        # context = torch.bmm(attn_weights, context.transpose(1, 2))
        # print("context:", context.size())
        return context

    def compute_loss(self, input_tensor, tags, seq_lens):
        mask = torch.zeros(input_tensor.shape[:2])
        mask = mask.to(device_lstm_crf_attn)
        mask = torch.greater(input_tensor, mask).type(torch.cuda.ByteTensor)
        # print("mask_shape_1:", mask)
        # print("tags:", tags.size())
        output_fc = self.forward(input_tensor, seq_lens)

        loss = -self.crf(output_fc, tags, mask, reduction='mean')
        return loss

    def decode(self, input_tensor, seq_lens):
        out = self.forward(input_tensor, seq_lens)
        mask = torch.zeros(input_tensor.shape[:2])
        mask = mask.to(device_lstm_crf_attn)
        mask = torch.greater(input_tensor, mask).type(torch.cuda.ByteTensor)

        predicted_index = self.crf.decode(out, mask)
        return predicted_index
